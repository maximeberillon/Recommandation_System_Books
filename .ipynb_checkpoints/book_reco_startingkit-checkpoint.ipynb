{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FlAXdTYP_NK"
   },
   "source": [
    "<img src='https://learnprimary.com.au/wp-content/uploads/2018/08/Girl-Choose-Book.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-smWdcG6f2v"
   },
   "source": [
    "<h1>Mimicking Amazon's Recommendation System for books<h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv6Is8Sv7BNm"
   },
   "source": [
    "*Youssef Brachmi, Théo Dullin, Omar El Mellouki, Lokmen Eltarr, Maxime Berillon*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nid5eQRTEBSo"
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ8HSjV68nI3"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "With the advent of technology, we witnessed a major outburst in social media contents and streaming services. The average person spends 3 hours and 15 minutes a day on its phone, more intensive users such as teenagers can spend up to 5 hours each day on their phones. One of the main collateral damages of all this content consumption is reading books. \n",
    "In 2018, a European survey showed that the average time spent reading books per day was 12 minutes in Finland and Poland, 7 minutes in Germany and Luxembourg, but only 2 min in France, the last in Europe.\n",
    "\n",
    "Many reasons can explain the lack of reading in the general population, but the main one is because reading a book is a long commitment that requires several hours, distributed in several successive days in order not to forget the context. This can prove to be very challenging for a majority of people and be a deterrent to start any book. Also, we don’t all have the time to go to a library and spend an hour reading back covers and choosing the right books.\n",
    "\n",
    "To cope with this, one has to choose wisely and only commit when sure that a book is interesting and deserves the time. Unfortunately, we don’t all have a relative or a friend that has read all books and advises us according to our preferences, but another way is using the same technology that distracts us from reading books, to encourage us reading books. \n",
    "\n",
    "Amazon, one of the biggest technology companies, started as a bookstore, an online one. From the comfort of your own home, you can order a book and receive it 2 days later in your mail. Another advantage of this online bookstore, is that users, readers in this case, can put reviews and notations for each book, with short comments to describe what they liked and disliked. This helps with book recommendations for each user, given his history of purchases and the various reviews for all books, a recommendation engine can produce a list of possible matches to what he would want next, thus having only a shortlist to choose from and spend more time in reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_XYg78lwdoM"
   },
   "source": [
    "## Scientific research \n",
    "\n",
    "In their article *Beyond Books: The Extended Academic Benefits of Library Use for First-Year College Students*  Soria et al. described how the use of library and most importantly book can have a positive impact on the academic outcomes of college students. Therefore it is crucial to advise them well.\n",
    "\n",
    "In another study called *A book reading intervention with preschool children who have limited vocabularies: the benefits of regular reading and dialogic reading*, Hargrave et al. focused on a younger generation and conclude in a similar way. They studied the influence of book reading to children who had poor vocabulary skills. These book reading interventions were very conclusive : the children exposes to these sessions rapidly gained vocabulary. This further stresses our point that book are vital for personal developement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix prize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5XxC1WypSJ0"
   },
   "source": [
    "<center><img src='https://media.wired.com/photos/5933082958b0d64bb35d419f/191:100/w_1280,c_limit/netflixprize.jpg'></center>\n",
    "\n",
    "In 2006 Netflix announced the creation of a special competition. The idea was simple : they provided a training dataset with user and reviews of movie and expected the participants to come up with a recommandation algorithm for their movies. The benchmark was one of their previous algorithm *Cinematch*. The leaderboard was based on the RMSE of each algorithm. In 2009, the winning team eventually won 1 million dollars !\n",
    "\n",
    "The idea here is quite similar but with books. And the prize is not 1 million dollars but 5 ECTS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmF7_CboOFJJ"
   },
   "source": [
    "# The challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-eD2iZecP3L"
   },
   "source": [
    "## Problem Desciption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hi85V9icPwv"
   },
   "source": [
    "The challenge that is proposed here is to build a book recommander system. Based on information about a list of books and the reviews of some users, the aim is to predict a list of book that a user is the most likely to like i,e, that he would review with a high rating.\n",
    "<br>\n",
    "We suppose that we have a set $U$ of users and a set $B$ of books available to recommand. If the user $u$ has rated the book $i$, then we denote $r_{ui}$ the rating given by $u$ to the book $i$. Of course, in the real life, all users does not review all books because they didn't read all books and because they don't review all the books that they read. <br> If we define $\\Omega=\\{(u,i) \\in U \\times B \\ \\text{such that u reviewed i}\\}$, we are looking to predict all $r_{ui}$ for $(u,i) \\notin \\Omega$. These prediction are called $(\\widehat{r}_{ui})_{(u,i)\\notin \\Omega}$.<br>\n",
    "\n",
    "*Important remark :* The rating of books on Amazon can only be 1,2,3,4 or 5. Thus, this looks like a classification problem. However, as we are interested into giving a list of most relevant books to a user. If a model gives the rating of 5 to a big list of books, it will be hard to discriminate books among this list and to recommand a small amount of books. Then, we are increasing the rating space by relaxing the discrete constrainte. The challenge proposed is then a regression problem that aims at predicting the rating in a continious space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1cyskut8rbN"
   },
   "source": [
    "## Description of the dataset\n",
    "\n",
    "The dataset used comes from the Amazon review data. It is a data set released in 2018 countaining reviews and metadata of products from May 1996 to Oct 2018. It is available here : [Amazon review data (2018)](https://nijianmo.github.io/amazon/index.html?fbclid=IwAR22w2ilC3jHIzAp_hnMsMLl8fiB6tpqJGxHWYqt5S8b5CCPqXqkgdH2lMQ#sample-metadata)\n",
    "The first version of the dataset were released in 2014, countained only reviews composed of ratings, text and helpfulness of votes. It is available here [Amazon review data (2014)](http://jmcauley.ucsd.edu/data/amazon/index_2014.html) The first dataset had 142.8 million reviews up to July 2014. The updated version used in this challenge adds more reviews (233.1 million reviews up to October 2018). It also adds metadata about products such as descriptions, category information, price, brand, and image features.\n",
    "<br>\n",
    "<br>\n",
    "The dataset is separated into categories of products sold on Amazon. The category that interests us is \"books\". It is composed of 51 311 621 reviews. \n",
    "<br>\n",
    "The number of books review is very big and can induce a lot of calculation time. Moreover, a lot of products have only been reviewed a small amount of time and a lot of users have only given a small amount of reviews. This implies that the behavior of people that gave a small amount of review will be difficult to catch. Furthermore, it will be hard to recommand books that have a small amount of review to other people.\n",
    "For all these reason, the number of data for this challenge has been reduced. We extract from the data the k-core, which means that each of the remaining users and items have k reviews each. We choose the integer k qualitatively to reduce significantly the size of the data while keeping a good amount of it. The k selected is : **10** which lead to a dataset of **4586** reviews. This leaves us with 320 user and 801 products.\n",
    "<br>\n",
    "<br>\n",
    "In the metadata, only the metadata associated to the selected reviews have been kept. All descriptions, category information, price and brand have been kept. For exmaple the image features have been dropped from the original dataset because the possibility to use it would complexify a lot the challenge and we would like it to be out of the scope of this challenge.\n",
    "<br>\n",
    "<br>\n",
    "The original data is on JSON format. After the preprocessing of the k-core, the data has been exported to the CSV format and correspond to the data available for this challenge.\n",
    "\n",
    "The data contains the following columns:\n",
    "\n",
    "- reviewerID — ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- productID — ID of the product, e.g. 0000013714\n",
    "- categories — list of categories the product belongs to\n",
    "- description — description of the product\n",
    "- title — name of the product\n",
    "- rank — sales rank information\n",
    "- brand — brand name\n",
    "- price — price in US dollars (at time of crawl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlcQvKNm8uku"
   },
   "source": [
    "## The cold-start problem\n",
    "\n",
    "The objective of this challenge is to accurately predict the target variable *ratingScore* based on the variables describing the previous purchases and reviews given by the user. The purpose of the study is to recommend relevant products that a user whose history of purchase is known might be interested in buying. We therefore consider that the previous products the user has bought will give an indication on those he would buy. \n",
    "\n",
    "However when a new user registers to the platform we have no prior information on his buying habits, and therefore cannot proceed to any inference since we cannot base it on any history. This issue, which is called the *Cold start for a new user*, is usually dealt with by asking the new user to provide some preferences to build an initial user profil, which will later be refined as the user uses the platform. However, a compromise must be made between the registration process and the amount of initial data required to build a profil, since the user might abandon the registration if its too long, and not enough data could lead to poor quality of recommendation.\n",
    "\n",
    "When a new product is added to the data base however it is possible to recommend it to known users based on its proximity to know products. We will see how in the second model approached here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "malIzMK58ynS"
   },
   "source": [
    "# Metrics\n",
    "\n",
    "In order to measure the accuracy of our recommendations, we need a measure. The function cost we have to minimize should be a distance between the user's reviews and our predictions for this reviews.\n",
    "\n",
    "Several metrics can be used to evaluate the performance of the book recommander. Here are a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyFlcE9FogZS"
   },
   "source": [
    "## MAE\n",
    "\n",
    "The *Mean Absolute Error* (MAE) has the same scale of the reviews themselves (from 1 to 5). If it equals 0.7 it means that on average the algorithm 0.7 point off.\n",
    "\n",
    "$$\n",
    "\\text{MAE} =\n",
    "\\frac{1}{n}\n",
    "\\sum_{(u,i) \\notin \\Omega}\n",
    "|\\hat{r}_{u,i}-r_{u,i}|\n",
    "$$\n",
    "\n",
    "The drawback of the MAE is that we cannot use it to compare results between datasets if the scale are different. That is shy we introduce the *Normalized Mean Absolute Error* (NMAE) :\n",
    "\n",
    "$$\n",
    "\\text{NMAE} =\n",
    "\\frac{1}{n(r_{high}-r_{low})}\n",
    "\\sum_{(u,i) \\notin \\Omega}\n",
    "|\\hat{r}_{u,i}-r_{u,i}|\n",
    "$$\n",
    "\n",
    "The MAE may be not enough for this problem. Generaly it tends to give more importance to large deviation compared to small deviation. Since NMAE is more balanced we want to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:26:35.985131Z",
     "start_time": "2021-01-17T12:26:35.975565Z"
    },
    "id": "RAUBsPk7o43Y"
   },
   "outputs": [],
   "source": [
    "def mae(a,b):\n",
    "    return np.mean(np.abs(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRCD5BSAompf"
   },
   "source": [
    "## MSE\n",
    "\n",
    "The *Mean Squared Error* (MSE) is similar but tends to penalize more larger errors of the recommendation system. For example it will penalize more one failure of 2 points that eight repeted failures of 0.5 point.\n",
    "\n",
    "$$\n",
    "\\text{MSE} =\n",
    "\\frac{1}{n}\n",
    "\\sum_{(u,i) \\notin \\Omega}\n",
    "(\\hat{r}_{u,i}-r_{u,i})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:26:35.999109Z",
     "start_time": "2021-01-17T12:26:35.989122Z"
    },
    "id": "smobQ9z3o7B2"
   },
   "outputs": [],
   "source": [
    "def mse(a,b):\n",
    "    return np.mean((a-b)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDEVnz2Tonqd"
   },
   "source": [
    "## RMSE\n",
    "Since we want to score how close are our predictions are to the ground truth, the first metric we will use is the root-mean-square error.\n",
    "\n",
    "The *Root Mean Squared Error* is the same as the MSE but with a square root. Like MAE it has the same units as the date. This measure of accuracy was chosen for the *Netflix Prize* and hence we will use it too to assess this challenge\n",
    "\n",
    "$$\n",
    "\\text{RMSE} =\n",
    "\\sqrt{\n",
    "\\frac{1}{n}\n",
    "\\sum_{(u,i) \\notin \\Omega}\n",
    "(\\hat{r}_{u,i}-r_{u,i})^2\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:26:35.968184Z",
     "start_time": "2021-01-17T12:26:35.949624Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse_f(a,b):\n",
    "    return np.sqrt(np.mean((a-b)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the RMSE will be our first metric and NMAE will be the second and we will consider both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acziaOhf82mT"
   },
   "source": [
    "\n",
    "# Data & imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2ZysSGkNmi6"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `ast`\n",
    "\n",
    "<br>\n",
    "You can install all requisite modules with pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:34:54.216669Z",
     "start_time": "2021-01-17T18:34:53.986590Z"
    },
    "id": "vPBbTJ-S0V92"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_IMwNnx26rS"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:23.013625Z",
     "start_time": "2021-01-17T18:41:22.972469Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "te0VSGTSOoBy",
    "outputId": "a700ab31-3f9e-4d00-adb4-93c87c63b2ee"
   },
   "outputs": [],
   "source": [
    "from problem import get_train_data, get_test_data\n",
    "X, y = get_train_data()\n",
    "y=y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vL_GtzkROhQ5"
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:23.403627Z",
     "start_time": "2021-01-17T18:41:23.397480Z"
    }
   },
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "data['ratingScore'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:23.604504Z",
     "start_time": "2021-01-17T18:41:23.586998Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27sec6k40bB1",
    "outputId": "2d1317bc-72a9-46fb-8097-aeef556461a9"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:23.785133Z",
     "start_time": "2021-01-17T18:41:23.774648Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of unique users: ',len(np.unique(data.loc[:,['reviewerID']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:24.023205Z",
     "start_time": "2021-01-17T18:41:24.012542Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of unique products: ',len(np.unique(data.loc[:,['productID']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:24.324419Z",
     "start_time": "2021-01-17T18:41:24.311641Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of possible reviews: ',len(np.unique(data.loc[:,['productID']].values))*len(np.unique(data.loc[:,['productID']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:24.564233Z",
     "start_time": "2021-01-17T18:41:24.550221Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of possible reviews: ',100-100*len(data)/(len(np.unique(data.loc[:,['productID']].values))*len(np.unique(data.loc[:,['productID']].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the sparsity of our user-product matrix is around 99% which makes the problem solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:30.938308Z",
     "start_time": "2021-01-17T18:41:25.058691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a heatmap of reviews and products ratings\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "colormap = plt.cm.coolwarm\n",
    "sns.heatmap(data.iloc[:,].pivot(index = 'reviewerID',\n",
    "                                columns = 'productID',\n",
    "                                values = 'ratingScore'),\n",
    "            cmap = colormap)\n",
    "plt.title('Heatmap of reviews and products ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a lot of sparcity, which is normal, not every user reviewed every single book, and no book is reviewed by all users.\n",
    "\n",
    "What is more interesting though is to see that the more a book is read, the higher its ratings, which we would suspect given that only good and famous books are read by a lot of people. We can also observe that some users are more severe than others, giving lower ratings in general.\n",
    "\n",
    "This entails us to see the distribution of average rating per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:30.982437Z",
     "start_time": "2021-01-17T18:41:30.941794Z"
    },
    "id": "tAdcdQBKn6QY"
   },
   "outputs": [],
   "source": [
    "# Add the mean rating per reviewer\n",
    "\n",
    "reviewer_rating = data.groupby(['reviewerID'])['ratingScore'].mean()\n",
    "data = pd.merge(data,reviewer_rating, how = 'left', on = 'reviewerID')\n",
    "data = data.rename(columns = {'ratingScore_x':'ratingScore',\n",
    "                              'ratingScore_y':'mean_rating_reviewer'})\n",
    "\n",
    "# Add the number of reviews per product\n",
    "\n",
    "product_rating = data.groupby(['productID'])['reviewerID'].count()\n",
    "data = pd.merge(data,product_rating, how = 'left', on = 'productID')\n",
    "data = data.rename(columns = {'reviewerID_x':'reviewerID',\n",
    "                              'reviewerID_y':'nb_reviews_product'})\n",
    "\n",
    "# Add the number of reviews per reviewers\n",
    "\n",
    "nb_reviews_reviewer = data.groupby(['reviewerID'])['ratingScore'].count()\n",
    "data = pd.merge(data,nb_reviews_reviewer, how = 'left', on = 'reviewerID')\n",
    "data = data.rename(columns = {'ratingScore_x':'ratingScore',\n",
    "                              'ratingScore_y':'nb_reviews_reviewer'})\n",
    "\n",
    "# Add the mean rating per product\n",
    "\n",
    "product_rating = data.groupby(['productID'])['ratingScore'].mean()\n",
    "data = pd.merge(data,product_rating, how = 'left', on = 'productID')\n",
    "data = data.rename(columns = {'ratingScore_x':'ratingScore',\n",
    "                              'ratingScore_y':'mean_rating_product'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:31.319014Z",
     "start_time": "2021-01-17T18:41:30.985814Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "crHV4ecLn6Wy",
    "outputId": "ef321497-3ee9-45f7-ba92-73f026690487"
   },
   "outputs": [],
   "source": [
    "# Distribution of the average rating per reviewer\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.hist(data['mean_rating_reviewer'],bins=30, color='#ff958f')\n",
    "plt.xlabel('Average rating per reviewer')\n",
    "plt.ylabel('Number of ratings per bin')\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "ax.set_title('Number of average ratings per reviewer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the average rating per reviewer is quite distributed, which we wouldn't suspect at first. Indeed, either some users seem to be more severe than others or they just didn't find enough interesting books for them. A majority of voters however seem to rate the books they read in a satisfactory fashion, giving a rating between 4 and 4.5. This could either mean that the users bypass the recommendations and only buy books which they are sure they will like, or the recommended products are well received. Coincidently, we also notice that some books were very poorly rated by the reviewers, showing either that the recommended books were not adapted (maybe due to the cold-start issue), or that the spontaneously purchased elements were disappointing.  \n",
    "\n",
    "The average rating per user is very important to our project. Given that we want to give the best recommendations to each reader, we have to maximize the expectancy of this average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:31.679468Z",
     "start_time": "2021-01-17T18:41:31.323885Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "-jpcevpBn6Z9",
    "outputId": "059cc3e4-ceed-4995-cb24-b3dbc08392f8"
   },
   "outputs": [],
   "source": [
    "# Distribution of the average rating per product\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "plt.hist(data['mean_rating_product'],bins=30, color='#ff958f')\n",
    "plt.xlabel('Average rating per product')\n",
    "plt.ylabel('Number of ratings per bin')\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "ax.set_title('Number of average ratings per product')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that the average rating per product is quite distributed as well, this is more expected but important for us. Even if the range of rating scale is from 1 to 5, only very few books are rated below 3, meaning that users generally read a book only when they suspect they will like it. \n",
    "\n",
    "Unlike other types of product, in which elements like the manufacturing quality, the durability, or the packaging, in the case of books the only element which will decide if a book is well rated or not (except in situations where their was an issue with the delivery or the transaction, therefore having nothing to do with the book in and of itself) is the appreciation of the reader of said book. Hence, the fact that some of the products were given bad ratings does not necessarily mean that they should be less put forward, but rather that they are recommended to the wrong category of customers. \n",
    "\n",
    "Another element which arises from the analysis of this plot is the correlation between the average rating of a product and the number of ratings said product has received. We often notice ourselves when shopping that the products which are highly rated have also received far more reviews than similar products, and are therefore heavily put forward by the platform. This element should not be neglected, as the comments and ratings given by other customers (and therefore their satisfaction with the product or lack thereof) are a critical factor in the final decision to buy or not. \n",
    "\n",
    "To better take into account the distribution of the ratings, one can normalize the ratings to improve some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:31.707952Z",
     "start_time": "2021-01-17T18:41:31.683334Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "id": "f_EtVTWJn6em",
    "outputId": "5305e34c-6226-4070-8375-79e95cc75362"
   },
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:34.829545Z",
     "start_time": "2021-01-17T18:41:34.814392Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jni_VsLCzKYE",
    "outputId": "b5c1f7b4-77b4-42a4-c232-399a2ef0b6d8"
   },
   "outputs": [],
   "source": [
    "# Transform rank feature from string to int\n",
    "\n",
    "def from_string_to_int(string):\n",
    "    return int(string.split()[0].replace(',',''))\n",
    "\n",
    "data['rank'] = data['rank'].apply(from_string_to_int)\n",
    "\n",
    "print('Ranks range from '+str(min(data['rank']))+\n",
    "      ' to '+str(max(data['rank'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranks are the one given by the website, it's based on an algorithm that combines ratings, number of ratings, visitors and quite a lot of other parameters. The ranks are also relative to a certain category of books, and a high rating could be achieved by narrowing enough the category in which the book falls.\n",
    "\n",
    "Before constructing a model based on this feature, one has to verify that it doesn't bias the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:46.005504Z",
     "start_time": "2021-01-17T18:41:45.600866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "1QyLmrZ_zN2W",
    "outputId": "ada7dc95-61e0-4666-8d32-5736e310c835"
   },
   "outputs": [],
   "source": [
    "# Plot correlation matrix\n",
    "\n",
    "corr = data.corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "heatmap = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1,\n",
    "                      annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap',\n",
    "                  fontdict={'fontsize':18}, pad=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprinsigly, the rank is not very correlated to the mean rating per reviewer nor the mean rating per product. Maybe the rank is based on some other unknown features.\\\n",
    "We notice that the rating is heavily correlated to the mean rating per product, which was to be expected. The rating score is also very correlated to the mean rating per reviewer. This element shows that which we highlighted earlier, namely that the users rate the products in a homogenous fashion. Again this could show that the customers only buy books which they are sure they will like, or they only leave reviews if they're satisfied by the products. Conversely, this could also mean that if a customers mean rating is low, then maybe he only leaves reviews on books which he was not satisfied with. \n",
    "\n",
    "To further explore this relationship between the ratings and the number of reviews we plot the subsequent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:41:48.642111Z",
     "start_time": "2021-01-17T18:41:48.324786Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "xmLAT7CZzH83",
    "outputId": "5bd6e037-ce98-45b2-eec1-b11e106e233b"
   },
   "outputs": [],
   "source": [
    "# Average rating per product with respect to its number of reviews\n",
    "\n",
    "fig, axis1 = plt.subplots(1,1,figsize=(10,6))\n",
    "plt.ylim(0,5.5)\n",
    "sns.scatterplot(x=\"nb_reviews_product\", y=\"mean_rating_product\",ax=axis1, data = data)\n",
    "axis1.axhline(y= data['ratingScore'].mean(), color='red')\n",
    "plt.xlabel('Number of reviews for a product')\n",
    "plt.ylabel('Mean rating per product')\n",
    "plt.title('Mean rating as a function of the number of reviews received')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the more reviews a product has, the higher its rating. This confirms our intuition that readers only read a book they think they'll like, and the most read books are generally the best ones and the highest rated. Nevertherless, books with lots of reviews are rated close to 4.2, not 5. \n",
    "\n",
    "This correlation however is not a causality. Indeed, the fact that a book has been reviewed many times does not necessarily mean that it is good, but points in that direction, as the number of reviews grows with the number of sales, which in turn mainly raise due to elements such as marketing, a cultural phenomenon (the release of a movie inspired from said book for instance), and of course the intrensic quality of the book. This should always be kept in mind, since those elements are impossible to predict or even quantify, and therefore useless when builind a machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:18.120792Z",
     "start_time": "2021-01-17T18:43:18.025862Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "GgXzufAYNKX4",
    "outputId": "83a9fcc6-772f-4590-ccb7-0dcfd7602291"
   },
   "outputs": [],
   "source": [
    "# Let's clean the data for further development of the models\n",
    "\n",
    "data['price'] = data['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "data['brand'] = data['brand'].replace(['Visit Amazon\\'s ','Page'],'', regex=True)\n",
    "data['category'] = data['category'].replace(['\\'Books\\', ',']','\\[','\\'','\\\"'],'', regex=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:34.745440Z",
     "start_time": "2021-01-17T18:43:34.726635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all book categories and number of their occurences in a sorted dictionary\n",
    "\n",
    "categories = data['category'].apply(lambda x: x.split(','))\n",
    "categories = [item for sublist in categories for item in sublist]\n",
    "categories = [item.strip() for item in categories]\n",
    "categories.remove('')\n",
    "\n",
    "occur = {}\n",
    "for i,item in enumerate(categories):\n",
    "    occur[item] = occur.get(item, 0) + 1\n",
    "occur['None'] = occur.pop('')\n",
    "occur = sorted(occur.items(), key=lambda x: x[1], reverse=True)\n",
    "occur = {item[0]: item[1] for item in occur}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:36.763933Z",
     "start_time": "2021-01-17T18:43:36.759007Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:40.674193Z",
     "start_time": "2021-01-17T18:43:40.093998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of the 30 first categories\n",
    "\n",
    "categories_1 = list(occur.keys())[:30]\n",
    "occurences_1 = list(occur.values())[:30]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.bar(categories_1,occurences_1)\n",
    "plt.xticks(rotation=85)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Number of books')\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "ax.set_title('Histogram of categories')\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(16) \n",
    "plt.show()\n",
    "\n",
    "# Get font sizes in plot to default\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows the categories by occurences. The majority of books are either from 'Litterature & Fiction', 'Mystery', or 'Thriller & Suspens'. Note that the categories are not mutually exclusive.\\\n",
    "What would be more important to see is the interactions between categories from books, so that for example if a lot of books from 'Thriller & Suspens' are also in 'Mystery', and some user likes 'Thriller & Suspens', we would also recommend books from 'Mystery'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SfI4ICC3Z-X"
   },
   "source": [
    "## A first model using SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we are going to use is part of the *model-based* approaches. These modeles were made famous during the Netflix Prize in 2006. They mainly rely on matrix factorization methods.\n",
    "\n",
    "If we denote $X \\in \\mathbb{R}^{n \\times m}$ our user-product matrix for $n$ clients and $m$ products where $x_{i,j}$ is the review of user $i$ for product $j$. This matrix is sparse : there are a lot of unknown coefficients in it. Our model will try to fill it basedd on what he already has.\n",
    "\n",
    "This thus becomes an imputation problem. We try to estimate a complete matrix $Z \\in \\mathbb{R}^{n \\times m}$ such that $Z \\approx X$. To make the problem easier let's assume $Z$ can be represented by a lower rank $k$ matrix such that $k \\ll min(m,n)$. Then we can find $Z \\approx V  G$ with $V \\in \\mathbb{R}^{n \\times k}$ and $G \\in \\mathbb{R}^{k \\times m}$.\n",
    "\n",
    "Let $\\Omega \\subset \\{1,...,n\\} \\times \\{1,...,m\\}$ be the index for whom $x_{i,j}$ is observed. Let's consider the following problem :\n",
    "\n",
    "$$\n",
    "\\min_{Z \\in \\mathbb{R}^{n \\times m}} \\text{rang}(Z) \\; \\;\n",
    "\\text{s.c.} \\sum_{(i,j) \\in \\Omega} (X_{i,j}-Z_{i,j})^2 \\leq \\delta\n",
    "$$\n",
    "\n",
    "The problem is too hard for $\\Omega \\neq \\emptyset$. Otherwise if $\\Omega = \\emptyset$ we can take :\n",
    "\n",
    "$$\n",
    "Z^k = U_k \\Sigma_k V_k^T\n",
    "$$\n",
    "\n",
    "with $U_k \\in \\mathbb{R}^{n \\times k}$ orthogonal, $V_k \\in \\mathbb{R}^{m \\times k}$ orthogonal and $\\Sigma \\in \\mathbb{R}^{k \\times k}$ such that $\\Sigma_k = diag(\\lambda_1,...,\\lambda_k)$.\n",
    "\n",
    "Let's consider an equivalent problem :\n",
    "\n",
    "$$\n",
    "\\min_{Z \\in \\mathbb{R}^{n \\times m}} \\left\\lVert Z \\right\\rVert_{*} \\; \\;\n",
    "\\text{s.c.} \\sum_{(i,j) \\in \\Omega} (X_{i,j}-Z_{i,j})^2 \\leq \\delta\n",
    "$$\n",
    "\n",
    "where $\\left\\lVert . \\right\\rVert_{*}$ is the nuclear norm.\n",
    "\n",
    "This problem is now convex thanks to the nuclear norm. Even thouhg it is simpler it reamins hard for large $X$ matrices.\n",
    "\n",
    "Now let's consider a final problem :\n",
    "\n",
    "$$\n",
    "\\min_{Z \\in \\mathbb{R}^{n \\times m}} \\frac{1}{2}\n",
    "\\sum_{(i,j) \\in \\Omega} (X_{i,j}-Z_{i,j})^2\n",
    "+ \\lambda \\left\\lVert Z \\right\\rVert_{*}\n",
    "$$\n",
    "\n",
    "This is a simpler problem that can be solved even with large matrices : $m$ et $n \\simeq 10^5-10^6$.\n",
    "\n",
    "We will resolve this problem with the `soft impute` algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required packages. For this we will use `IterativeImputer` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:55.533794Z",
     "start_time": "2021-01-17T18:43:55.529898Z"
    },
    "id": "512L2lNR26BA"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build our user-product matrix, here it's `ratings_matrix`. `IterativeImputer` requires a 2D array so we have to convert our matrix, here it's `ratings_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:43:56.019788Z",
     "start_time": "2021-01-17T18:43:55.898275Z"
    },
    "id": "6SmB3uu-2mo0"
   },
   "outputs": [],
   "source": [
    "ratings_matrix = data.pivot_table(values='ratingScore', index='productID', columns='reviewerID', fill_value=0)\n",
    "ratings_array = ratings_matrix.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the method `fit_transform` the algorithm fills our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:17.431003Z",
     "start_time": "2021-01-17T18:43:56.285046Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78oJe4bx2tjY",
    "outputId": "f7241f79-150b-4c31-945f-c11b3bcb4e9a"
   },
   "outputs": [],
   "source": [
    "imp_mean = IterativeImputer(missing_values=0, random_state=0, min_value=1, max_value=5)\n",
    "ratings_array_predicted = imp_mean.fit_transform(ratings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:17.485261Z",
     "start_time": "2021-01-17T18:44:17.437625Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep9oyBlq3VUm",
    "outputId": "8dbbdc21-59fd-403d-c887-990e5ac30cfd"
   },
   "outputs": [],
   "source": [
    "ratings_array_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the method implemented in our starting kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:17.499595Z",
     "start_time": "2021-01-17T18:44:17.488969Z"
    }
   },
   "outputs": [],
   "source": [
    "def svd_prediction(matrix, array, mean, u, i):\n",
    "    '''\n",
    "    Given a user-product matrix and the corresponding 2D array\n",
    "    Return the predicted ratingScore\n",
    "    If its a new user or a new product it returns the mean ratingScore\n",
    "    '''\n",
    "    if i in matrix.index:\n",
    "        idx_i = matrix.index.get_loc(i)\n",
    "        idx_u = matrix.columns.get_loc(u)\n",
    "        pred = array[idx_i][idx_u]\n",
    "    else:\n",
    "        pred = mean\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:17.527393Z",
     "start_time": "2021-01-17T18:44:17.513686Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.imp = IterativeImputer(missing_values=0,\n",
    "                                    random_state=0,\n",
    "                                    min_value=1,\n",
    "                                    max_value=5)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.mean_ratingScore = np.mean(y)\n",
    "        data = X.loc[:, ['reviewerID', 'productID']]\n",
    "        data['ratingScore'] = y\n",
    "        self.ratings_matrix = data.pivot_table(values='ratingScore',\n",
    "                                               index='productID',\n",
    "                                               columns='reviewerID',\n",
    "                                               fill_value=0)\n",
    "        ratings_array = self.ratings_matrix.values\n",
    "        self.ratings_array_predicted = self.imp.fit_transform(ratings_array)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [svd_prediction(self.ratings_matrix,\n",
    "                                 self.ratings_array_predicted,\n",
    "                                 self.mean_ratingScore,\n",
    "                                 x[0], x[1]) for x in X.values]\n",
    "        y_pred=np.array(y_pred)\n",
    "        return y_pred.reshape((y_pred.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:36.243684Z",
     "start_time": "2021-01-17T18:44:17.531047Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Regressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:36.271814Z",
     "start_time": "2021-01-17T18:44:36.255078Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:44:36.305287Z",
     "start_time": "2021-01-17T18:44:36.276290Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_f(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOMqnkms8-AT"
   },
   "source": [
    "## Second model introducing hybrid method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method we will present is hybrid: it mixes an Item-KNN with a content-based approach.\n",
    "\n",
    "We define two types of similarity between products:\n",
    "- An item-based similarity: if two items are rated in the same way then they are close\n",
    "- A content-based similarity: if two items are defined by the same categories then they are close\n",
    "\n",
    "**Content-based similarity**\n",
    "\n",
    "To define the later we need to vectorize the products in terms of categories. We will achieve that by doing Multi Hot Encoding. In the example below we have:\n",
    "\n",
    "- A belongs to 2 categories: fantasy and Arts\n",
    "- B belongs to 3 categories: History, Arts and Litterature\n",
    "- C belongs to 1 category: Fantasy\n",
    "- D does not belong to any category\n",
    "\n",
    "Product | Fantasy | History | Arts | Litterature\n",
    "------- | ------- | ------- | ---- | -----------\n",
    "A | 1 | 0 | 1 | 0\n",
    "B | 0 | 1 | 1 | 1\n",
    "C | 1 | 0 | 0 | 0\n",
    "D | 0 | 0 | 0 | 0\n",
    "\n",
    "Let's consider $u$ and $v$ that corresponds to the categories of product $i$ and $j$. The cosine similarity fo these two products are defined by:\n",
    "\n",
    "$$\n",
    "\\text{CosSim}(i,j)\n",
    "=\\frac\n",
    "{\\sum_{k=1}^M u_k v_k}\n",
    "{\n",
    "\\sqrt{\\sum_{k=1}^M u_k^2}\n",
    "\\sqrt{\\sum_{k=1}^M v_k^2}\n",
    "}\n",
    "$$\n",
    "\n",
    "**Item-based similarity**\n",
    "\n",
    "The item-based similarity is computed with the scores of each product.\n",
    "\n",
    "$$\n",
    "\\text{ScoSim}(i,j) =\\frac{\\sum_{u \\in \\mathcal{U}_{i,j}} (r_{u,i}-\\overline{r_i}) (r_{u,j}-\\overline{r_j})}{\\sqrt{\\sum_{u \\in \\mathcal{U}_{i,j}} (r_{u,i}-\\overline{r_i})^2 \\times \\sum_{u \\in \\mathcal{U}_{i,j}} (r_{u,j}-\\overline{r_j})^2}}\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "- $r_{u,i}$ the review given by $u$ for product $i$\n",
    "- $\\overline{r_i}$ the average review of product $i$\n",
    "- $\\mathcal{U}_{i,j}$ the set of user that both reviewed $i$ and $j$\n",
    "\n",
    "**Combined similarity**\n",
    "\n",
    "We can finally define the combined similarity that mixes the two approaches:\n",
    "\n",
    "$$\n",
    "\\text{CombSim}(i,j) = \\alpha \\: \\text{SemSim}(i,j) + (1-\\alpha) \\: NotSim(i,j)\n",
    "$$\n",
    "\n",
    "with $\\alpha$ a parameter to tune.\n",
    "\n",
    "**Predicting the review**\n",
    "\n",
    "Let $\\mathcal{N}_{u}^{k}(i)$ be the k nearest neighbors of $i$ reviewed by $u$ as defines by the similarity measure. We can now compute the prediction for product $i$ and user $u$:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i} = \\frac\n",
    "{\n",
    "\\sum_{j \\in \\mathcal{N}_{u}^{k}(i)} w_{i,j} r_{u,j}\n",
    "}\n",
    "{\n",
    "\\sum_{j \\in \\mathcal{N}_{u}^{k}(i)} |w_{i,j}|\n",
    "}\n",
    "$$\n",
    "\n",
    "Note that the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: this methods won't be included in a submission file, the main point here is to show what is possible with this dataset. The following paragraph is to be seen as a possible reflexion pist for a better model*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T17:42:10.433081Z",
     "start_time": "2021-01-17T17:42:10.378963Z"
    }
   },
   "source": [
    "First a little preprocessing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:45.026772Z",
     "start_time": "2021-01-17T13:43:45.008017Z"
    }
   },
   "outputs": [],
   "source": [
    "df = X_train.loc[:,['reviewerID','productID','category']].drop_duplicates().reset_index(drop=True)\n",
    "df['ratingScore'] = y_train\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the category data is rather messy since we have a string and not a list of list... Even though we already show in the exploratory analysis how to deal with that type of issue let's see another way of doing things.\n",
    "\n",
    "First we create `cat_list`: a list of list with every category of each item. Then we create `cat_list_ohe`: a list of arrays that represent each product categories as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:45.728487Z",
     "start_time": "2021-01-17T13:43:45.454551Z"
    },
    "id": "6O9hISawLRCH"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "cat_list = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    cat_sublist_raw = ast.literal_eval(df.iloc[i,2])\n",
    "    try:\n",
    "        cat_sublist_raw.remove('Books')\n",
    "    except:\n",
    "        pass\n",
    "    cat_sublist = []\n",
    "    for item in cat_sublist_raw:\n",
    "        new_item = item.replace('&amp;', '&')\n",
    "        cat_sublist.append(new_item)\n",
    "    cat_list.append(cat_sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:45.737684Z",
     "start_time": "2021-01-17T13:43:45.731208Z"
    }
   },
   "outputs": [],
   "source": [
    "unique_cat_list = list(set([item for sublist in cat_list for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:46.284512Z",
     "start_time": "2021-01-17T13:43:46.275525Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(unique_cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.332851Z",
     "start_time": "2021-01-17T13:43:46.641271Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_list_ohe = []\n",
    "\n",
    "for item in cat_list:\n",
    "    if not item:\n",
    "        ohe_item = np.zeros(shape=len(unique_cat_list), dtype=np.int8)\n",
    "    else:\n",
    "        ohe_item = sum(lb.transform(item))\n",
    "    cat_list_ohe.append(ohe_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is done we define the cosine similarity as well as the ratings similarity. These will be used to compute the combinedd similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.353092Z",
     "start_time": "2021-01-17T13:43:48.340276Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine(u,v):\n",
    "    num = np.sum(u * v)\n",
    "    denom = (np.sqrt(np.sum(u))) * (np.sqrt(np.sum(v)))\n",
    "    if not denom:\n",
    "        return 0\n",
    "    else:\n",
    "        res = num/denom\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.367632Z",
     "start_time": "2021-01-17T13:43:48.361891Z"
    }
   },
   "outputs": [],
   "source": [
    "def user_average(u):\n",
    "    ratings = ratings_matrix.iloc[u].values\n",
    "    mean = ratings[ratings != 0].mean()\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.393254Z",
     "start_time": "2021-01-17T13:43:48.372616Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(i, j):\n",
    "    list_products = df.loc[:,'productID'].values\n",
    "    idx_i = np.where(list_products==i)[0][0]\n",
    "    idx_j = np.where(list_products==j)[0][0]\n",
    "    vec_i = cat_list_ohe[idx_i]\n",
    "    vec_j = cat_list_ohe[idx_j]\n",
    "    return cosine(vec_i, vec_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.408308Z",
     "start_time": "2021-01-17T13:43:48.398195Z"
    }
   },
   "outputs": [],
   "source": [
    "def ratings_similarity(i, j):\n",
    "    product_list_rate = ratings_matrix.index.tolist()\n",
    "    nbr_columns = len(ratings_matrix.columns)\n",
    "    num, denum1, denum2 = 0.0, 0.0, 0.0\n",
    "    if (i in product_list_rate)&(j in product_list_rate):\n",
    "        vect_i = ratings_matrix.loc[i].values\n",
    "        vect_j = ratings_matrix.loc[j].values\n",
    "        for k in range(nbr_columns):\n",
    "            num += (vect_i[k]-user_average(k))*(vect_j[k]-user_average(k))\n",
    "            denum1 += (vect_i[k]-user_average(k))**2\n",
    "            denum2 += (vect_j[k]-user_average(k))**2\n",
    "        coef = num/np.sqrt(denum1*denum2)\n",
    "        return coef\n",
    "    else:\n",
    "        return float('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.419199Z",
     "start_time": "2021-01-17T13:43:48.412845Z"
    }
   },
   "outputs": [],
   "source": [
    "def combined_similarity(i, j, alpha):\n",
    "    return alpha * cosine_similarity(i,j) + (1-alpha) * ratings_similarity(i,j) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with $\\alpha = 0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:43:48.654517Z",
     "start_time": "2021-01-17T13:43:48.429413Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_similarity('0007133766','0006159990',0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work like a breeze. We can now proceed and create the function that will generate the predictions. We can tune $\\alpha$ and $k$ the number of neighbors selected in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:57:30.251625Z",
     "start_time": "2021-01-17T13:57:30.243657Z"
    }
   },
   "outputs": [],
   "source": [
    "def itemknn_prediction_f(u, i, alpha=0.2, k=5):\n",
    "    also_rated = df[df['reviewerID']==u]['productID'].values\n",
    "    sim = {}\n",
    "    for j in also_rated:\n",
    "        if not combined_similarity(i, j, alpha):\n",
    "            sim[j] = 0\n",
    "        else:\n",
    "            sim[j] = combined_similarity(i, j, alpha)\n",
    "    knn = sorted(sim, key=sim.get, reverse=True)[:k]\n",
    "    num = 0.0\n",
    "    denum = 0.0\n",
    "    for j in knn:\n",
    "        rating_j = int(df[(df['reviewerID']==u)&(df['productID']==j)]['ratingScore'].values)\n",
    "        num += rating_j * combined_similarity(i, j, alpha)\n",
    "        denum += combined_similarity(i, j, alpha)\n",
    "    if denum == 0.0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = num/denum\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T13:44:39.577615Z",
     "start_time": "2021-01-17T13:44:27.687711Z"
    }
   },
   "outputs": [],
   "source": [
    "itemknn_prediction_f('A5S98AEI9WI7Y','0006512461')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works perfectly as well. Now let's use it on some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T17:52:50.653187Z",
     "start_time": "2021-01-17T17:51:25.359025Z"
    }
   },
   "outputs": [],
   "source": [
    "c = 10\n",
    "\n",
    "y_pred = []\n",
    "X_test , y_test = get_test_data()\n",
    "\n",
    "for index, row in X_test[:c].iterrows():\n",
    "    y_pred.append(itemknn_prediction_f(row[0],row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T17:52:50.706675Z",
     "start_time": "2021-01-17T17:52:50.675450Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_f(y_test[:c],y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is not bad and is actually lower than the one we had on the SVD model. Of course we should tune the parameters and do some cross validation but that's not bad at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKdQfcE_8_fi"
   },
   "source": [
    "# Testing the submission locally\n",
    "\n",
    "Once you have come up with a good model, we advise to test it locally. Do to so, the package ramp_workflow needs to be installed. The package is in the list of requirements.txt and can be installed by the following command if it is not already installed.\n",
    "\n",
    "```\n",
    "python -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/master\n",
    "```\n",
    "\n",
    "Your code should be written in a python file that you name estimator.py. It has to countain a function name get_estimator that returns an object of type scikit-learn like that solves the problem. <br>\n",
    "This file should be found in a folder called anyway you want, itself found in the **submission/** folder.\n",
    "As a example, you can find the SVD model of this notbook in the path submission\\SVD\\estimator.py. <br>\n",
    "<br>\n",
    "\n",
    "Your code can be tested on the data set with the folowing command :\n",
    "```\n",
    "ramp-test --submission <your submission folder name>\n",
    "```\n",
    "A Ramp-workflow option is also implemented in the problem that allow you to run your code on a smaller set of the data. This allow you to test quicker your code on a subset. The command to run your code with the quick-mode is the following : \n",
    "```\n",
    "ramp-test --submission <your submission folder name> --quick-test\n",
    "```\n",
    "\n",
    "For example, to run the SVD model on quick mode, you can run the following line :\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T12:27:07.754318Z",
     "start_time": "2021-01-17T12:27:05.186861Z"
    },
    "id": "pJ_shz6cauyq"
   },
   "outputs": [],
   "source": [
    "! ramp-test --submission starting_kit --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEuIGGWYZzhQ"
   },
   "source": [
    "# More information\n",
    "For more information on how to submit your code on [ramp.studio](https://ramp.studio/), refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html).\n",
    "You can also read the READ.ME file"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SZ8HSjV68nI3",
    "B_XYg78lwdoM",
    "P5XxC1WypSJ0",
    "t1cyskut8rbN",
    "dlcQvKNm8uku",
    "yKdQfcE_8_fi"
   ],
   "name": "Data Camp Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
